ğŸ“¦ **AI-Powered Dependency Risk Analyzer**
An AI-assisted software supply chain security tool that analyzes package.json files, evaluates dependency risks, enforces security policies, and generates human-readable explanations using a local LLM with safe fallback mechanisms.
This project is designed with production-grade resilience, ensuring security decisions remain deterministic even when AI is slow or unavailable.

## Why This Project Exists ğŸš€ 
Modern applications rely heavily on third-party dependencies.
A single vulnerable dependency can compromise the entire system.
This tool helps by:
Detecting high-risk dependencies
Enforcing security escalation policies
Producing clear explanations for security decisions
Using AI responsibly, never as a single point of failure

## Key Features ğŸ§  
**ğŸ” Dependency Risk Analysis**
Parses dependencies and devDependencies
Detects known vulnerabilities
Assigns risk scores and severity levels


##  Policy & Escalation Engine ğŸš¨
Automatically BLOCKS, WARNS, or ALLOWS dependencies
Rule-based decision system for deterministic behavior

##  AI-Generated Security Explanations ğŸ¤–
Uses local LLM (Ollama + Llama 3)
Generates concise, professional explanations
Explains why a decision was made, not just what

##  Safe AI Fallback (Production-Grade)ğŸ›¡ï¸
If AI takes longer than a defined timeout:
Automatically falls back to rule-based explanations
Guarantees no request ever hangs
Guarantees no AI dependency for security decisions

 ## Demo Mode (Recruiter Friendly)ğŸ¯
Allows controlled AI generation for demos
Proves AI capability without risking instability

##  System Architecture (High Level)ğŸ—ï¸

Client (package.json upload)
        â†“
Dependency Analyzer
        â†“
Risk Scoring Engine
        â†“
Policy & Escalation Rules
        â†“
Final Decision (BLOCK / WARN / ALLOW)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Explanationâ”‚  â† Ollama (Llama 3)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (timeout or error)
Rule-Based Explanation (Guaranteed)

ğŸ”‘ Security decisions never depend on AI availability

 ## API Endpoints ğŸ“¡
âœ… Health Check

POST /health
Response

Json
{
  "ok": true,
  "service": "dependency-risk-analyzer",
  "message": "Server is healthy"
}

## Analyze DependenciesğŸ“¦

POST /analyze

Input
Multipart form-data
Key: file
Value: package.json

Response
Json
{
  "success": true,
  "analysisId": "uuid",
  "finalDecision": {
    "action": "BLOCK"
  },
  "summary": {
    "totalDependencies": 7,
    "riskSeverity": "HIGH"
  }
}


ğŸ¤– Get Explanation

POST /explain/:analysisId

Optional demo mode:

POST /explain/:analysisId?mode=demo

AI Response

Json
{
  "success": true,
  "explanation": {
    "version": "v2",
    "source": "AI",
    "explanation": "The final decision was to block..."
  },
  "aistatus": "AI"
}

Fallback Response

Json
{
  "success": true,
  "explanation": {
    "version": "v1",
    "source": "RULE_BASED"
  },
  "aistatus": "RULE_BASED"
}


## AI Design Philosophy (Important) ğŸ§  

This project follows Responsible AI principles:
AI is used for explanations only
Security decisions are always deterministic
AI failures never break the system
Timeouts ensure predictable performance
AI enhances understanding â€” it never replaces policy enforcement.

 ## Tech Stack ğŸ› ï¸
Backend: Node.js, Express
AI: Ollama (Llama 3 â€“ local inference)
Security Logic: Custom rule & escalation engine
Storage: In-memory store (extensible to Redis/DB)
Deployment Ready: Docker (coming next)

ğŸ§ª Example Use Case
Upload a package.json
System detects high-risk dependency (lodash, minimist)
Policy engine blocks the dependency
AI generates a security-focused explanation
If AI is slow â†’ fallback explanation is returned instantly

##  Future Enhancements ğŸ”®
Docker Compose (API + Ollama)
CI/CD pipeline
Persistent storage (Redis / MongoDB)
SBOM export
GitHub dependency scanning integration

##  Author ğŸ§‘â€ğŸ’»
Abhinav Anand
Full-Stack Developer | Security-Focused Backend Engineer
AI-Driven Systems | Supply Chain Security
